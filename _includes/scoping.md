---
---

This is not intended to be an introductory guide to these systems, but focused on making sense of what new search tools are providing and what they might become. These reviews may be useful to heavy users, developers, and others looking to understand changes in system support for various searching practices.

I will largely be looking at systems for web search, including those more focused to particular subject areas. Though important, these reviews will not (yet at least) engage with new search systems for:

- academic search:
    - Examples:
        - Consensus ([consensus.app](https://consensus.app/))
        - Elicit ([elicit.org](https://elicit.org/))
    - See: 
        - Michael Gusenbauer's [call for independent audits](https://www.nature.com/articles/d41586-023-01613-w) ([author copy](https://twitter.com/michaelgusenb/status/1658773090346598401))
        - Aaron Tay's ["categorization of interesting new academic discovery tools"](https://twitter.com/aarontay/status/1676274770307088384)
- enterprise search
    - Examples:
        - Glean ([glean.com](https://www.glean.com/))
        - Vectara ([vectara.com](https://vectara.com/))
- personal knowledge management
    - Examples:
        - Klu ([klu.so](https://klu.so/))
        - Rewind ([rewind.ai](https://www.rewind.ai/))

I will not be trying to replicate metrics like that in [ragas](https://github.com/explodinggradients/ragas) (tagline: "Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines"; from [Exploding Gradient](https://github.com/explodinggradients)). Those metrics are intended to examine: Faithfulness, Context Relevancy, Context Recall, Answer Relevancy, Aspect Critiques. I'm more interested in actions outside the RAG-pipeline itself. What can be done to improve coordination between searcher expectation and system performance? What can be done to remedy system failures at searching-time?

I will also not be focused on in-editor code generation tools (like GitHub's Copilot) and writing tools (like Lex.page) that replace or subsume some searching tasks.

I will not very focused on various metrics related to speed, unless it is very noticeable in frequent use.

I am concerned about questions of bias, but here only insofar as these systems are markedly different from the prior problems found in search.

I am not focused on explainability or transparency of these systems, though some question will definitely engage with those questions. I will be more focused on examining questions around seamfulness, tractability, and traceability. I will be thinking about how practical algorithmic knowledge [@cotter2022practical] is built up and valued.

I'm less focused on responding to or rehashing and regurgitating arguments about "model collapse", than perhaps looking at how these search tools and their users imagine supporting or working towards [*unsealing knowledge*, whether through articulations that help users doubt & dig deeper, providing multiple drafts, or RAG adaptations](/ppost/2023/09/06/unsealing-knowledge.html).

The most important work would be work looking at how these search systems and tools are imagined and used (or not) by other people. I am not looking at that right now, but I will look at aspects of the systems identified publicly by different users or others.