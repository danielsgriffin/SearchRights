@article{burrell2019control,
  author = {Burrell, Jenna and Kahn, Zoe and Jonas, Anne and Griffin, Daniel},
  title = {When Users Control the Algorithms: Values Expressed in Practices on {Twitter}},
  year = {2019},
  issue_date = {November 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {CSCW},
  URL={https://doi.org/10.1145/3359240},
  doi = {10.1145/3359240},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  month = nov,
  articleno = {Article 138},
  numpages = {20},
  keywords = {assembly, automation, control, gaming the algorithm, algorithmic fairness, human autonomy, twitter}
  }

@phdthesis{griffin2022situating,
  title={Situating Web Searching in Data Engineering: Admissions, Extensions, Repairs, and Ownership},
  author={Griffin, Daniel},
  year={2022},
  school={University of California, Berkeley},
  URL={https://danielsgriffin.com/assets/griffin2022situating.pdf}
  }

@article{griffin2022search,
  author = {Daniel Griffin and Emma Lurie},
  title ={Search quality complaints and imaginary repair: Control in articulations of {Google Search}},
  journal = {New Media \& Society},
  volume = {0},
  number = {0},
  pages = {14614448221136505},
  year = {2022},
  doi = {10.1177/14614448221136505},
  
  URL={https://doi.org/10.1177/14614448221136505},
  eprint = {
  https://doi.org/10.1177/14614448221136505
  
  }
  ,
  abstract = { In early 2017, a journalist and search engine expert wrote about “Google’s biggest ever search quality crisis.” Months later, Google hired him as the first Google “Search Liaison” (GSL). By October 2021, when someone posted to Twitter a screenshot of misleading Google Search results for “had a seizure now what,” users tagged the Twitter account of the GSL in reply. The GSL frequently publicly interacts with people who complain about Google Search on Twitter. This article asks: what functions does the GSL serve for Google? We code and analyze 6 months of GSL responses to complaints on Twitter. We find that the three functions of the GSL are: (1) to naturalize the logic undergirding Google Search by defending how it works, (2) perform repair in responses to complaints, and (3) boundary drawing to control critique. This advances our understanding of how dominant technology companies respond to critiques and resist counter-imaginaries. }
  }

@article{metaxa2021auditing,
  URL={http://dx.doi.org/10.1561/1100000083},
  year = {2021},
  volume = {14},
  journal = {Foundations and Trends® in Human–Computer Interaction},
  title = {Auditing Algorithms: Understanding Algorithmic Systems from the Outside In},
  doi = {10.1561/1100000083},
  issn = {1551-3955},
  number = {4},
  pages = {272-344},
  author = {Danaë Metaxa and Joon Sung Park and Ronald E. Robertson and Karrie Karahalios and Christo Wilson and Jeff Hancock and Christian Sandvig}
  }

@article{lam2022end,
  author = {Lam, Michelle S. and Gordon, Mitchell L. and Metaxa, Dana\"{e} and Hancock, Jeffrey T. and Landay, James A. and Bernstein, Michael S.},
  title = {End-User Audits: A System Empowering Communities to Lead Large-Scale Investigations of Harmful Algorithmic Behavior},
  year = {2022},
  issue_date = {November 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {6},
  number = {CSCW2},
  URL={https://doi.org/10.1145/3555625},
  doi = {10.1145/3555625},
  abstract = {Because algorithm audits are conducted by technical experts, audits are necessarily limited to the hypotheses that experts think to test. End users hold the promise to expand this purview, as they inhabit spaces and witness algorithmic impacts that auditors do not. In pursuit of this goal, we propose end-user audits-system-scale audits led by non-technical users-and present an approach that scaffolds end users in hypothesis generation, evidence identification, and results communication. Today, performing a system-scale audit requires substantial user effort to label thousands of system outputs, so we introduce a collaborative filtering technique that leverages the algorithmic system's own disaggregated training data to project from a small number of end user labels onto the full test set. Our end-user auditing tool, IndieLabel, employs these predicted labels so that users can rapidly explore where their opinions diverge from the algorithmic system's outputs. By highlighting topic areas where the system is under-performing for the user and surfacing sets of likely error cases, the tool guides the user in authoring an audit report. In an evaluation of end-user audits on a popular comment toxicity model with 17 non-technical participants, participants both replicated issues that formal audits had previously identified and also raised previously underreported issues such as under-flagging on veiled forms of hate that perpetuate stigma and over-flagging of slurs that have been reclaimed by marginalized communities.},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  month = {nov},
  articleno = {512},
  numpages = {34},
  keywords = {algorithmic fairness, machine learning, algorithm auditing, human-centered ai, interactive visualization}
  }

@article{cotter2022practical,
  doi = {10.1177/14614448221081802},
  URL={https://doi.org/10.1177/14614448221081802},
  year = {2022},
  month = mar,
  publisher = {{SAGE} Publications},
  pages = {1--20},
  author = {Kelley Cotter},
  title = {Practical knowledge of algorithms: The case of {BreadTube}},
  journal = {New Media \& Society}
  }